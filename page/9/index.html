<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  




<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="sunxing&apos;s blog | mysql | java | scala | hadoop | spark | kafka | flume">
<meta property="og:type" content="website">
<meta property="og:title" content="孙星的个人博客~">
<meta property="og:url" content="http://www.sunxing.cc/page/9/index.html">
<meta property="og:site_name" content="孙星的个人博客~">
<meta property="og:description" content="sunxing&apos;s blog | mysql | java | scala | hadoop | spark | kafka | flume">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="孙星的个人博客~">
<meta name="twitter:description" content="sunxing&apos;s blog | mysql | java | scala | hadoop | spark | kafka | flume">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>
 <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258995301'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258995301' type='text/javascript'%3E%3C/script%3E"));</script>

  <title> 孙星的个人博客~ </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">孙星的个人博客~</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">搬砖的,码农~</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/07/hbase-study003/" itemprop="url">
                  HBase学习笔记(三) HBase的shell操作
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-07T11:55:55+08:00" content="2016-06-07">
              2016-06-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hbase/" itemprop="url" rel="index">
                    <span itemprop="name">hbase</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/07/hbase-study003/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/07/hbase-study003/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>1 查看表</p>
<p><img src="/images/hbase/hbase-list-shell.png" alt=""></p>
<p>2 创建表,查看表,插入数据</p>
<p><img src="/images/hbase/hbase-shell.png" alt=""></p>
<p>3 查询数据</p>
<p><img src="/images/hbase/hbase-get.png" alt=""></p>
<p>get命令操作的主要顺序是:表名称,行名称,列名称,如果有其他的调教在用花括号加上,具体的代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">hbase&gt; get ‘t1′, ‘r1′ </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, &#123;TIMERANGE =&gt; [ts1, ts2]&#125; </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, &#123;COLUMN =&gt; ‘c1′&#125; </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, &#123;COLUMN =&gt; [&apos;c1&apos;, &apos;c2&apos;, &apos;c3&apos;]&#125; </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, &#123;COLUMN =&gt; ‘c1′, TIMESTAMP =&gt; ts1&#125; </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, &#123;COLUMN =&gt; ‘c1′, TIMERANGE =&gt; [ts1, ts2], VERSIONS =&gt; 4&#125; </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, &#123;COLUMN =&gt; ‘c1′, TIMESTAMP =&gt; ts1, VERSIONS =&gt; 4&#125; </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, ‘c1′ </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, ‘c1′, ‘c2′ </div><div class="line">hbase&gt; get ‘t1′, ‘r1′, [&apos;c1&apos;, &apos;c2&apos;]</div></pre></td></tr></table></figure>
<p>scan 命令查询表数据</p>
<p>这个命令主要是扫描表数据,可根据给定的条件进行数据查询并展示.</p>
<p><img src="/images/hbase/hbase-scan-example.png" alt=""></p>
<p>4 删除数据</p>
<p><img src="/images/hbase/hbase-delete.png" alt=""></p>
<p>如果需要进行全表删除操作，就使用truncate命令，其实没有直接的全表删除命令，这个命令也是disable，drop，create三个命令组合出来的</p>
<p>5 修改表</p>
<p>增加一个列簇:</p>
<p><img src="/images/hbase/hbase-alter.png" alt=""></p>
<p>删除一个列簇：</p>
<p><img src="/images/hbase/hbase-alter-delete.png" alt=""></p>
<p>执行多个修改动作</p>
<p><img src="/images/hbase/hbase-alter-mult.png" alt=""></p>
<p>6 统计数据</p>
<p>统计表中数据的函数,根据指定条件查询</p>
<p><img src="/images/hbase/hbase-count.png" alt=""></p>
<p>count一般会比较耗时，使用mapreduce进行统计，统计结果会缓存，默认是10行。统计间隔默认的是1000行（INTERVAL）</p>
<p>7 表的可用和隐藏</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">//隐藏表</div><div class="line">disable tableName</div><div class="line"></div><div class="line">//表可用</div><div class="line">enbale tableName</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/01/hbase-study001/" itemprop="url">
                  HBase学习笔记(一) HBase的集群环境搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-01T18:55:55+08:00" content="2016-06-01">
              2016-06-01
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hbase/" itemprop="url" rel="index">
                    <span itemprop="name">hbase</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/01/hbase-study001/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/01/hbase-study001/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-集群环境部署"><a href="#1-集群环境部署" class="headerlink" title="1 集群环境部署"></a>1 集群环境部署</h2><p>hbase集群环境是通过zookeeper来进行协调管理调度的,也就是说搭建集群环境需要有zookeeper,在hbase中集成了zookeeper,可以通过配置文件中的参数来实现控制,参数的详细说明如下:</p>
<p><img src="/images/hbase/hbase-zk-config.png" alt=""></p>
<p>这个参数在hbase-env.sh中,由上面的注释可知这个是告诉hbase应该去管理哪一个zk实例。</p>
<h2 id="2-zookeeper的配置安装"><a href="#2-zookeeper的配置安装" class="headerlink" title="2 zookeeper的配置安装"></a>2 zookeeper的配置安装</h2><p>1 下载zookeeper的安装包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">wget http://mirrors.cnnic.cn/apache/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz</div><div class="line"></div><div class="line">tar -zxvf zookeeper-3.4.8.tar.gz</div></pre></td></tr></table></figure>
<p>2 修改zookeeper的配置：</p>
<p><img src="/images/hbase/zookeeper-conf.png" alt=""></p>
<p>3 给每个节点增加对应的id编号,并且和配置文件中对应</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat &quot;1&quot; &gt;&gt; data/myid</div></pre></td></tr></table></figure>
<p>在不同的节点上修改不同的id参数,配置信息。</p>
<p>4 将配置好的zookeeper迁移到其他机器上去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scp zookeeper3.4.8 root@hadoop-slave:/data/</div></pre></td></tr></table></figure>
<p>5 启动相应的zookeeper</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /data/zookeeper3.4.8</div><div class="line"></div><div class="line">./bin/zkServer.sh start</div></pre></td></tr></table></figure>
<h2 id="3-hbase的安装"><a href="#3-hbase的安装" class="headerlink" title="3 hbase的安装"></a>3 hbase的安装</h2><p>hbase-site.xml的配置信息如下:</p>
<p><img src="/images/hbase/hbase-site.png" alt=""></p>
<p>将配置好的hbase拷贝到多台机器上去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scp hbase1.2.0 root@hadoop-slave:/data/</div></pre></td></tr></table></figure>
<p>启动hbase:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /data/hbase1.2.0</div><div class="line"></div><div class="line">./bin/start-hbase.sh</div></pre></td></tr></table></figure>
<p>hbase运行起来后的进程如下:</p>
<p>主节进程如下:</p>
<p><img src="/images/hbase/hbase-master.png" alt=""></p>
<p>从节点的进程如下:</p>
<p><img src="/images/hbase/hbase-node.png" alt=""></p>
<p>由于测试环境在跑数据，所以会有一些yarn的进程在。hbase运行正常的话,在主节点有进程:HMaster,从节点会有:HRegionServer。</p>
<h2 id="4-安装过程遇到的问题"><a href="#4-安装过程遇到的问题" class="headerlink" title="4 安装过程遇到的问题"></a>4 安装过程遇到的问题</h2><p>1 时间同步问题</p>
<p>异常截图如下：</p>
<p><img src="/images/hbase/hbase-error-clock.png" alt=""></p>
<p>主要原因是因为hbase集群中各个节点中时间不同步,解决方案如下:</p>
<pre><code>a 允许各个节点间时间存在差异,指定差异的大小,设定方式如：
&lt;property&gt;
    &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;
    &lt;value&gt;180000&lt;/value&gt;
    &lt;description&gt;Time difference of regionserver from master&lt;/description&gt;
&lt;/property&gt;

b 或者同步各个节点的数据

 安装工具
 yum install ntp
 安装后执行
 ntpdate cn.pool.ntp.org
 即可同步国际时间..
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/01/hbase-study002/" itemprop="url">
                  HBase学习笔记(二) HBase的简单介绍(转)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-01T18:55:55+08:00" content="2016-06-01">
              2016-06-01
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hbase/" itemprop="url" rel="index">
                    <span itemprop="name">hbase</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/01/hbase-study002/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/01/hbase-study002/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-表-行-列和单元格"><a href="#1-表-行-列和单元格" class="headerlink" title="1 表,行,列和单元格"></a>1 表,行,列和单元格</h2><p>hbase中最基本的单位是单元格,其中一张表中有多行,一行可以有多个列族,一个列族由多个列组成，具体的结构图如下:</p>
<p><img src="/images/hbase/hbase-table.png" alt=""></p>
<p>1) RowKey: 行键，可理解成MySQL中的主键列。</p>
<p>2) Column: 列，可理解成MySQL列。</p>
<p>3) ColumnFamily: 列族, HBase引入的概念：<br>    将多个列聚合成一个列族。<br>    可以理解成MySQL的垂直分区（将一张宽表，切分成几张不那么宽的表）。<br>    此机制引入的原因，是因为HBase相信，查询可能并不需要将一整行的所有列数据全部返回。（就像我们往往在写SQL时不    太会写select all一样）<br>    对应到文件存储结构（不同的ColumnFamily会写入不同的文件）。</p>
<p>4) TimeStamp：在每次跟新数据时，用以标识一行数据的不同版本（事实上，TimeStamp是与列绑定的。）</p>
<h2 id="2-hbase系统架构图"><a href="#2-hbase系统架构图" class="headerlink" title="2 hbase系统架构图"></a>2 hbase系统架构图</h2><p><img src="/images/hbase/hbase-jiagou.jpg" alt=""></p>
<p>组成部件说明</p>
<p>Client:</p>
<pre><code>使用HBase RPC机制与HMaster和HRegionServer进行通信 
Client与HMaster进行通信进行管理类操作 
Client与HRegionServer进行数据读写类操作 
</code></pre><p>Zookeeper： </p>
<pre><code>Zookeeper Quorum存储-ROOT-表地址、HMaster地址 
HRegionServer把自己以Ephedral方式注册到Zookeeper中，HMaster随时感知各个HRegionServer的健康状况 
Zookeeper避免HMaster单点问题 
</code></pre><p>HMaster： </p>
<pre><code>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master在运行 
主要负责Table和Region的管理工作： 
    1 管理用户对表的增删改查操作 
    2 管理HRegionServer的负载均衡，调整Region分布 
    3 Region Split后，负责新Region的分布 
    4 在HRegionServer停机后，负责失效HRegionServer上Region迁移 
</code></pre><p>HRegionServer： </p>
<pre><code>HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据
</code></pre><p><img src="/images/hbase/hbase-region.jpg" alt=""></p>
<p>自动分区:</p>
<p>HBase中负责扩张和负载均衡的基本单元成为region,region本质上是以行键排序的连续存储的分区,</p>
<p>如果region太大,系统就会自动的进行拆分,相反的,如果region太小的话,系统就会进行合并,以减少数据的存储文件数量。</p>
<p>一张表初始化的时候只有一个region,当用户向其中插入数据的时候,系统会自动检测这个region的大小是否超过配置的最大值,如果超过这个限制,系统会在中间键(region中间哪一行)处将这个region拆分两个等大小的子region。</p>
<p>HRegionServer:</p>
<pre><code>每个HRegion对应Table中一个Region，HRegion由多个HStore组成； 
每个HStore对应Table中一个Column Family的存储； 
Column Family就是一个集中的存储单元，故将具有相同IO特性的Column放在一个Column Family会更高效
</code></pre><p>HStore</p>
<pre><code>HBase存储的核心,由MemStore和StoreFile组成
MemStore是Sorted Memory Buffer，由用户写入数据的流程
</code></pre><p><img src="/images/hbase/hbase-hstore.gif" alt=""></p>
<p>Client写入 -&gt; 存入MemStore，一直到MemStore满 -&gt; Flush成一个StoreFile，直至增长到一定阈值 -&gt; 出发Compact合并操作 -&gt; 多个StoreFile合并成一个StoreFile，同时进行版本合并和数据删除 -&gt; 当StoreFiles Compact后，逐步形成越来越大的StoreFile -&gt; 单个StoreFile大小超过一定阈值后，触发Split操作，把当前Region Split成2个Region，Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer上，使得原先1个Region的压力得以分流到2个Region上</p>
<p>由此过程可知，HBase只是增加数据，有所得更新和删除操作，都是在Compact阶段做的，所以，用户写操作只需要进入到内存即可立即返回，从而保证I/O高性能。</p>
<p>HLog</p>
<p>引入HLog原因：</p>
<p>在分布式系统环境中，无法避免系统出错或者宕机，一旦HRegionServer以外退出，MemStore中的内存数据就会丢失，引入HLog就是防止这种情况</p>
<p>工作机制：<br>每个HRegionServer中都会有一个HLog对象，HLog是一个实现Write Ahead Log的类，每次用户操作写入Memstore的同时，也会写一份数据到HLog文件，HLog文件定期会滚动出新，并删除旧的文件(已持久化到StoreFile中的数据)。当HRegionServer意外终止后，HMaster会通过Zookeeper感知，HMaster首先处理遗留的HLog文件，将不同region的log数据拆分，分别放到相应region目录下，然后再将失效的region重新分配，领取到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>
<p>HBase存储格式</p>
<p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，格式主要有两种：</p>
<p>1 HFile HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile</p>
<p>2 HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence Filed<br>I<br><img src="/images/hbase/hbase-hfile.jpg" alt=""></p>
<p>图片解释：<br>HFile文件不定长，长度固定的块只有两个：Trailer和FileInfo<br><br>Trailer中指针指向其他数据块的起始点<br><br>File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等<br><br>Data Index和Meta Index块记录了每个Data块和Meta块的起始点<br><br>Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制<br><br>每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询<br><br>每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏<br></p>
<p>HFile里面的每个KeyValue对就是一个简单的byte数组。这个byte数组里面包含了很多项，并且有固定的结构</p>
<p><img src="/images/hbase/hbase-hf.jpg" alt=""></p>
<p>KeyLength和ValueLength：两个固定的长度，分别代表Key和Value的长度<br><br>Key部分：Row Length是固定长度的数值，表示RowKey的长度，Row 就是RowKey<br><br>Column Family Length是固定长度的数值，表示Family的长度<br><br>接着就是Column Family，再接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）<br><br>Value部分没有这么复杂的结构，就是纯粹的二进制数据<br></p>
<p><img src="/images/hbase/hbase-hlog.jpg" alt=""></p>
<p>HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。<br>HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/27/hive-study005/" itemprop="url">
                  Hive学习笔记(五) Hive自定义函数开发
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-27T18:55:55+08:00" content="2016-05-27">
              2016-05-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hive/" itemprop="url" rel="index">
                    <span itemprop="name">hive</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/27/hive-study005/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/27/hive-study005/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-UDF函数的编写"><a href="#1-UDF函数的编写" class="headerlink" title="1 UDF函数的编写"></a>1 UDF函数的编写</h2><p>主要是在解析数据的时候可能我们拿到的数据是一个json,当然在hive中有解析json的内置函数,但是我们的数据不是很规范而且还是json嵌套json的格式,因此想实现一个给定一个路径,也就是我需要抽取那个路径下的数据的字段对应的字比如：</p>
<pre><code>{
    &quot;a&quot;:[
        {
            &quot;age&quot;:1
        },
        {
            &quot;age&quot;:2
        }
    ],
    &quot;b&quot;:&quot;test&quot;
}
</code></pre><p>这个是乱写的测试数据,如果我想获取指定的数据如： a.0.age,这个是表示age:1的那个值,也就是获取的结果应该为1.主要的做法就是解析json.</p>
<p>具体的代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line">import org.apache.hadoop.hive.ql.exec.UDF;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.json.JSONArray;</div><div class="line">import org.json.JSONObject;</div><div class="line"></div><div class="line">public class UDFTool extends UDF &#123;</div><div class="line">    /**</div><div class="line">     * 格式化抽取数据</div><div class="line">     *</div><div class="line">     * @param s</div><div class="line">     * @param parrent</div><div class="line">     * @return</div><div class="line">     */</div><div class="line">    public String evaluate(final Text s, String parrent) &#123;</div><div class="line">        //数据判断</div><div class="line">        if (s == null) return null;</div><div class="line">        try &#123;</div><div class="line">            //格式化数据为json对象</div><div class="line">            JSONObject bean = new JSONObject(s.toString());</div><div class="line"></div><div class="line">            return UDFTool.cast(bean, parrent.toString().split(&quot;\\.&quot;), 0).toString();</div><div class="line">        &#125; catch (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">        return null;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 格式化抽取字符串数组</div><div class="line">     *</div><div class="line">     * @param s</div><div class="line">     * @param path</div><div class="line">     * @param split</div><div class="line">     * @return</div><div class="line">     */</div><div class="line">    public String evaluate(final Text s, final String path, String split) &#123;</div><div class="line">        //数据判断</div><div class="line">        if (s == null) return null;</div><div class="line">        try &#123;</div><div class="line">            //格式化数据为json对象</div><div class="line">            JSONObject bean = new JSONObject(s.toString());</div><div class="line"></div><div class="line">            //获取json字符串的值</div><div class="line">            String array = UDFTool.cast(bean, path.toString().split(&quot;\\.&quot;), 0).toString();</div><div class="line"></div><div class="line">            //格式化为jsonarray</div><div class="line">            JSONArray arr = new JSONArray(array);</div><div class="line"></div><div class="line">            //遍历jsonarray</div><div class="line">            StringBuffer buffer = new StringBuffer();</div><div class="line">            for (int i = 0; i &lt; arr.length(); i++) &#123;</div><div class="line">                buffer.append(arr.get(i).toString());</div><div class="line">                if (i + 1 &lt; arr.length()) &#123;</div><div class="line">                    buffer.append(split);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            return buffer.toString();</div><div class="line">        &#125; catch (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">        return null;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    /**</div><div class="line">     * 格式化数据格式</div><div class="line">     *</div><div class="line">     * @param bean</div><div class="line">     * @param key</div><div class="line">     * @return</div><div class="line">     */</div><div class="line">    public static Object cast(Object bean, String key) &#123;</div><div class="line">        try&#123;</div><div class="line">            if (bean instanceof JSONObject) &#123;</div><div class="line">                if (((JSONObject) bean).has(key)) &#123;</div><div class="line">                    return ((JSONObject) bean).get(key);</div><div class="line">                &#125;</div><div class="line">                return null;</div><div class="line">            &#125; else if (bean instanceof JSONArray) &#123;</div><div class="line">                JSONArray ar = (JSONArray) bean;</div><div class="line">                int index = -1;</div><div class="line">                try &#123;</div><div class="line">                    index = Integer.parseInt(key);</div><div class="line">                &#125; catch (Exception e) &#123;</div><div class="line">                    index = -1;</div><div class="line">                &#125;</div><div class="line">                if (index &lt; 0 || index &gt; ar.length() - 1) return null;</div><div class="line">                return ar.get(index);</div><div class="line">            &#125; else &#123;</div><div class="line">                return bean;</div><div class="line">            &#125;</div><div class="line">        &#125;catch (Exception e)&#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">        return null;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 根据给定的格式解析json</div><div class="line">     *</div><div class="line">     * @param bean</div><div class="line">     * @param key</div><div class="line">     * @param index</div><div class="line">     * @return</div><div class="line">     */</div><div class="line">    public static Object cast(Object bean, String[] key, int index) &#123;</div><div class="line">        if (index == key.length - 1) return cast(bean, key[index]);</div><div class="line">        return cast(cast(bean, key[index]), key, index + 1);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="2-UDAF"><a href="#2-UDAF" class="headerlink" title="2 UDAF"></a>2 UDAF</h2><pre><code>•Hive查询数据时，有些聚类函数在HQL没有自带，需要用户自定义实现
•用户自定义聚合函数: Sum, Average…… n – 1
•UDAF（User- Defined Aggregation Funcation） 
</code></pre><p>用法</p>
<pre><code>•一下两个包是必须的import org.apache.hadoop.hive.ql.exec.UDAF和org.apache.hadoop.hive.ql.exec.UDAFEvaluator
</code></pre><p>开发步骤</p>
<pre><code>•函数类需要继承UDAF类，内部类Evaluator实UDAFEvaluator接口
•Evaluator需要实现 init、iterate、terminatePartial、merge、terminate这几个函数
    a）init函数实现接口UDAFEvaluator的init函数。
    b）iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean。
    c）terminatePartial无参数，其为iterate函数轮转结束后，返回轮转数据，terminatePartial类似于hadoop的Combiner。
    d）merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。
    e）terminate返回最终的聚集函数结果。
</code></pre><p>执行步骤</p>
<pre><code>•执行求平均数函数的步骤
    a）将java文件编译成Avg_test.jar。
    b）进入hive客户端添加jar包：
        hive&gt;add jar /run/jar/Avg_test.jar。
    c）创建临时函数：
        hive&gt;create temporary function avg_test &apos;hive.udaf.Avg&apos;;
    d）查询语句：
        hive&gt;select avg_test(scores.math) from scores;
    e）销毁临时函数：
        hive&gt;drop temporary function avg_test;
</code></pre><p>UDAF代码示例</p>
<pre><code>public class MyAvg extends UDAF {

public static class AvgEvaluator implements UDAFEvaluator {}
public void init() {}
public boolean iterate(Double o) {}
public AvgState terminatePartial() {}
public boolean terminatePartial(Double o) { }
public Double terminate() {}}
</code></pre><h2 id="3-UDTF"><a href="#3-UDTF" class="headerlink" title="3 UDTF"></a>3 UDTF</h2><p>开发步骤</p>
<pre><code>•UDTF步骤：
•必须继承org.apache.Hadoop.hive.ql.udf.generic.GenericUDTF
•实现initialize, process, close三个方法
•UDTF首先会
•调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型） 
初始化完成后，会调用process方法，对传入的参数进行处理，可以通过forword()方法把结果返回
•最后close()方法调用，对需要清理的方法进行清理
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</div><div class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</div><div class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</div><div class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</div><div class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</div><div class="line">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</div><div class="line">import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</div><div class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</div><div class="line"></div><div class="line">import java.util.ArrayList;</div><div class="line">import java.util.Iterator;</div><div class="line">import java.util.List;</div><div class="line"></div><div class="line">public class UDTFTool extends GenericUDTF &#123;</div><div class="line">    private PrimitiveObjectInspector stringOI = null;</div><div class="line">    private static final String SPLIT_STR = &quot;udtf_hive_tool&quot;;</div><div class="line"></div><div class="line">    @Override</div><div class="line">    public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException &#123;</div><div class="line"></div><div class="line">        if (args.length != 1) &#123;</div><div class="line">            throw new UDFArgumentException(&quot;NameParserGenericUDTF() takes exactly one argument&quot;);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        if (args[0].getCategory() != ObjectInspector.Category.PRIMITIVE</div><div class="line">                &amp;&amp; ((PrimitiveObjectInspector) args[0]).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING) &#123;</div><div class="line">            throw new UDFArgumentException(&quot;NameParserGenericUDTF() takes a string as a parameter&quot;);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        // 输入格式（inspectors）</div><div class="line">        stringOI = (PrimitiveObjectInspector) args[0];</div><div class="line"></div><div class="line">        // 输出格式（inspectors） -- 有两个属性的对象</div><div class="line">        List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</div><div class="line">        List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</div><div class="line">        fieldNames.add(&quot;value&quot;);</div><div class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</div><div class="line"></div><div class="line">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public ArrayList&lt;Object[]&gt; processInputRecord(String name)&#123;</div><div class="line">        ArrayList&lt;Object[]&gt; result = new ArrayList&lt;Object[]&gt;();</div><div class="line"></div><div class="line">        // 忽略null值与空值</div><div class="line">        if (name == null || name.isEmpty()) &#123;</div><div class="line">            return result;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String[] tokens = name.split(SPLIT_STR);</div><div class="line"></div><div class="line">        if(tokens == null || tokens.length &lt;= 0)return result;</div><div class="line">        for(int i=0;i&lt;tokens.length;i++)&#123;</div><div class="line">            result.add(new Object[]&#123;tokens[i]&#125;);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        return result;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    @Override</div><div class="line">    public void process(Object[] record) throws HiveException &#123;</div><div class="line"></div><div class="line">        final String name = stringOI.getPrimitiveJavaObject(record[0]).toString();</div><div class="line"></div><div class="line">        ArrayList&lt;Object[]&gt; results = processInputRecord(name);</div><div class="line"></div><div class="line">        Iterator&lt;Object[]&gt; it = results.iterator();</div><div class="line"></div><div class="line">        while (it.hasNext())&#123;</div><div class="line">            Object[] r = it.next();</div><div class="line">            forward(r);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    @Override</div><div class="line">    public void close() throws HiveException &#123;</div><div class="line">        // do nothing</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/27/hive-study004/" itemprop="url">
                  Hive学习笔记(四) Hive参数
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-27T16:31:55+08:00" content="2016-05-27">
              2016-05-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hive/" itemprop="url" rel="index">
                    <span itemprop="name">hive</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/27/hive-study004/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/27/hive-study004/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Hive-参数"><a href="#Hive-参数" class="headerlink" title="Hive 参数"></a>Hive 参数</h2><p>hive.exec.max.created.files</p>
<pre><code>•说明：所有hive运行的map与reduce任务可以产生的文件的和
•默认值:100000 
</code></pre><p>hive.exec.dynamic.partition</p>
<pre><code>•说明：是否为自动分区
•默认值：false
</code></pre><p>hive.mapred.reduce.tasks.speculative.execution</p>
<pre><code>•说明：是否打开推测执行
•默认值：true
</code></pre><p>hive.input.format</p>
<pre><code>•说明：Hive默认的input format
•默认值： org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
•如果有问题可以使用org.apache.hadoop.hive.ql.io.HiveInputFormat
</code></pre><p>hive.exec.counters.pull.interval</p>
<pre><code>•说明：Hive与JobTracker拉取counter信息的时间
•默认值：1000ms 
</code></pre><p>hive.script.recordreader</p>
<pre><code>•说明：使用脚本时默认的读取类
•默认值： org.apache.hadoop.hive.ql.exec.TextRecordReader
</code></pre><p>hive.script.recordwriter</p>
<pre><code>•说明：使用脚本时默认的数据写入类
•默认值： org.apache.hadoop.hive.ql.exec.TextRecordWriter
</code></pre><p>hive.mapjoin.check.memory.rows</p>
<pre><code>•说明： 内存里可以存储数据的行数
•默认值： 100000
</code></pre><p>hive.mapjoin.smalltable.filesize</p>
<pre><code>•说明：输入小表的文件大小的阀值，如果小于该值，就采用普通的join
•默认值： 25000000
</code></pre><p>hive.auto.convert.join</p>
<pre><code>•说明：是不是依据输入文件的大小，将Join转成普通的Map Join
•默认值： false
</code></pre><p>hive.mapjoin.followby.gby.localtask.max.memory.usage</p>
<pre><code>•说明：map join做group by 操作时，可以使用多大的内存来存储数据，如果数据太大，则不会保存在内存里
•默认值：0.55
</code></pre><p>hive.mapjoin.localtask.max.memory.usage</p>
<pre><code>•说明：本地任务可以使用内存的百分比
•默认值： 0.90
</code></pre><p>hive.heartbeat.interval</p>
<pre><code>•说明：在进行MapJoin与过滤操作时，发送心跳的时间
•默认值1000
</code></pre><p>hive.merge.size.per.task</p>
<pre><code>•说明： 合并后文件的大小
•默认值： 256000000
</code></pre><p>hive.mergejob.maponly</p>
<pre><code>•说明： 在只有Map任务的时候 合并输出结果
•默认值： true
</code></pre><p>hive.merge.mapredfiles</p>
<pre><code>•默认值： 在作业结束的时候是否合并小文件
•说明： false
</code></pre><p>hive.merge.mapfiles</p>
<pre><code>•说明：Map-Only Job是否合并小文件
•默认值：true
</code></pre><p>hive.hwi.listen.host</p>
<pre><code>•说明：Hive UI 默认的host
•默认值：0.0.0.0
</code></pre><p>hive.hwi.listen.port</p>
<pre><code>•说明：Ui监听端口
•默认值：9999
</code></pre><p>hive.exec.parallel.thread.number</p>
<pre><code>•说明：hive可以并行处理Job的线程数
•默认值：8
</code></pre><p>hive.exec.parallel</p>
<pre><code>•说明：是否并行提交任务
•默认值：false
</code></pre><p>hive.exec.compress.output</p>
<pre><code>•说明：输出使用压缩
•默认值： false
</code></pre><p>hive.mapred.mode</p>
<pre><code>•说明： MapReduce的操作的限制模式，操作的运行在该模式下没有什么限制
•默认值： nonstrict
</code></pre><p>hive.join.cache.size</p>
<pre><code>•说明： join操作时，可以存在内存里的条数
•默认值： 25000
</code></pre><p>hive.mapjoin.cache.numrows</p>
<pre><code>•说明： mapjoin 存在内存里的数据量
•默认值：25000
</code></pre><p>hive.join.emit.interval</p>
<pre><code>•说明： 有连接时Hive在输出前，缓存的时间
•默认值： 1000
</code></pre><p>hive.optimize.groupby</p>
<pre><code>•说明：在做分组统计时，是否使用bucket table
•默认值： true
</code></pre><p>hive.fileformat.check</p>
<pre><code>•说明：是否检测文件输入格式
•默认值：true
</code></pre><p>hive.metastore.client.connect.retry.delay</p>
<pre><code>•说明： client 连接失败时,retry的时间间隔
•默认值：1秒
</code></pre><p>hive.metastore.client.socket.timeout</p>
<pre><code>•说明:  Client socket 的超时时间
•默认值：20秒
</code></pre><p>mapred.reduce.tasks</p>
<pre><code>•默认值：-1
•说明：每个任务reduce的默认值
     -1 代表自动根据作业的情况来设置reduce的值 
</code></pre><p>hive.exec.reducers.bytes.per.reducer</p>
<pre><code>•默认值： 1000000000 （1G）
•说明：每个reduce的接受的数据量
        如果送到reduce的数据为10G,那么将生成10个reduce任务 
</code></pre><p>hive.exec.reducers.max</p>
<pre><code>•默认值：999
•说明： reduce的最大个数      
</code></pre><p>hive.exec.reducers.max</p>
<pre><code>•默认值：999
•说明： reduce的最大个数
</code></pre><p>hive.metastore.warehouse.dir</p>
<pre><code>•默认值：/user/hive/warehouse
•说明： 默认的数据库存放位置
</code></pre><p>hive.default.fileformat</p>
<pre><code>•默认值：TextFile
•说明： 默认的fileformat
</code></pre><p>hive.map.aggr</p>
<pre><code>•默认值：true
•说明： Map端聚合，相当于combiner
</code></pre><p>hive.exec.max.dynamic.partitions.pernode</p>
<pre><code>•默认值：100
•说明：每个任务节点可以产生的最大的分区数
</code></pre><p>hive.exec.max.dynamic.partitions</p>
<pre><code>•默认值：1000
•说明： 默认的可以创建的分区数
</code></pre><p>hive.metastore.server.max.threads</p>
<pre><code>•默认值：100000
•说明： metastore默认的最大的处理线程数
</code></pre><p>hive.metastore.server.min.threads</p>
<pre><code>•默认值：200
•说明： metastore默认的最小的处理线程数
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
           <!-- 多说热评文章 start -->
	<div class="ds-top-threads" data-range="daily" data-num-items="5"></div>
<!-- 多说热评文章 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"sunxing"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->


           <section id="comments">
   <!-- 多说评论框 start -->
<div id="ds-thread" class="ds-thread" data-thread-key="<%= post.path %>" data-title="<%= post.title %>" data-url="<%= post.permalink %>"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"sunxing"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->
  </section>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/default_avatar.jpg"
               alt="孙星" />
          <p class="site-author-name" itemprop="name">孙星</p>
          <p class="site-description motion-element" itemprop="description">sunxing's blog | mysql | java | scala | hadoop | spark | kafka | flume</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">65</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">孙星</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"sunxing"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  

  

  
<script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js">
</script>


</body>
</html>
