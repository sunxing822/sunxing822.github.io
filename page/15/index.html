<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  




<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="sunxing&apos;s blog | mysql | java | scala | hadoop | spark | kafka | flume">
<meta property="og:type" content="website">
<meta property="og:title" content="孙星的个人博客~">
<meta property="og:url" content="http://www.sunxing.cc/page/15/index.html">
<meta property="og:site_name" content="孙星的个人博客~">
<meta property="og:description" content="sunxing&apos;s blog | mysql | java | scala | hadoop | spark | kafka | flume">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="孙星的个人博客~">
<meta name="twitter:description" content="sunxing&apos;s blog | mysql | java | scala | hadoop | spark | kafka | flume">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>
 <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258995301'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258995301' type='text/javascript'%3E%3C/script%3E"));</script>

  <title> 孙星的个人博客~ </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">孙星的个人博客~</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">搬砖的,码农~</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/09/hive-study001/" itemprop="url">
                  Hive学习笔记(一) Hive环境搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-09T18:39:55+08:00" content="2016-05-09">
              2016-05-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hive/" itemprop="url" rel="index">
                    <span itemprop="name">hive</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/09/hive-study001/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/09/hive-study001/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>Hive相当于是一个客户端,抽象的mysql客户端,而这个客户端对应的数据存储是存在HDFS中,Hive中相应的sql解析器主要是将sql语言解析封装成为一个可执行的MapReduce作业.</p>
</blockquote>
<p>Hive的元数据存储模式一共有三种:</p>
<ul>
<li>内嵌metastore</li>
<li>本地metastore</li>
<li>远程metastore</li>
</ul>
<p>由于内嵌的metastore不是很稳定,hive的原始配置文件中数据源的配置为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</div><div class="line">    &lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt;</div><div class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</div><div class="line">  &lt;/property&gt;</div></pre></td></tr></table></figure>
<p>Apache Derby是一个完全用java编写的数据库，Derby是一个Open source的产品,Apache Derby非常小巧，核心部分derby.jar只有2M，所以既可以做为单独的数据库服务器使用，也可以内嵌在应用程序中使用。但是不稳定,针对hive使用中不是很适合。因此这里使用mysql用作元数据的存储。</p>
<h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><p>hive 安装版本使用的是1.2.1，安装模式采用的是本地模式，其中，安装文件夹路径如下：</p>
<p><img src="/images/hive/first.png" alt=""></p>
<p>安装方式:<br>1 解压hive1.2.1的安装包<br>2 修改配置文件</p>
<p><img src="/images/hive/conf-list.png" alt=""></p>
<p>在hive中配置文件中主要是hive-default.xml.template，以及hive-env.sh.template<br>其中hive中有要求，如果hive-default.xml和hive-site.xml同时存在的时候的，如果hive-site.xml和hive-default.xml中的key有相同的话<br>以hive-site.xml中为准。因此创建了一个hive-site.xml文件.</p>
<p>3 首先需要修改的是环境变量，配置hadoop的路径以及hive的配置文件路径，如果需要积累hive的日志文件记录，需要修改hive-log4j.properties<br>修改的内容如下:<br>hive-env.sh</p>
<p><img src="/images/hive/env-conf.png" alt=""></p>
<p>log4j日志配置<br>hive-log4j.properties</p>
<p><img src="/images/hive/log-conf.png" alt=""></p>
<p>hive-site.xml的配置详细如下:</p>
<p>​<img src="/images/hive/hive-site.png" alt=""><br>​<br>​对以上配置信息进行初步的说明:</p>
<ul>
<li>javax.jdo.option.ConnectionURL                        数据库的链接</li>
<li>javax.jdo.option.ConnectionDriverName                 数据库驱动名称</li>
<li>javax.jdo.option.ConnectionUserName                   登陆数据库的用户名称</li>
<li>javax.jbo.option.ConnectionPassword                   登陆用户的密码</li>
<li>hive.exec.local.scratchdir                            hive本地粗糙你的一些临时文件</li>
<li>hive.metastore.warehouse.dir                            hive的数据库存储的路径</li>
<li>hive.exec.mode.local.auto                                  是否是本地模式</li>
<li>hive.querylog.location                                hive查询操作的日志目录</li>
</ul>
<h2 id="Hive安装后出现的问题"><a href="#Hive安装后出现的问题" class="headerlink" title="Hive安装后出现的问题"></a>Hive安装后出现的问题</h2><p>1 向hive中创建数据库并插入到一条数据，抛异常：Specified key was too long; max key length is 767 bytes</p>
<pre><code>原因： 主要是hive使用的是本地的mysql，由于hive用到的数据库是手动创建的，编码格式不对，因此需要手动更改数据对应的编码格式
首先需要在数据库中更改对应的数据库的编码格式:
alter database hive character set latin1;
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/09/hadoop-study003/" itemprop="url">
                  Hadoop学习笔记(三) MapReduce框架详解
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-09T17:13:55+08:00" content="2016-05-09">
              2016-05-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">hadoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/09/hadoop-study003/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/09/hadoop-study003/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>MapReduce是一个计算框架,有输入数据源和数据结果源,通过定义好的计算模型将输入数据源进行计算,得出相应的数据结果<br>在运行一个mapreduce计算任务时候,任务主要分为2个阶段：map阶段和reduce阶段,每个阶段都是通过kv键值对座位输入和输出的,计算模型主要就是编写相对应的map和reduce函数</p>
</blockquote>
<p>从mapreduce运行机制的各个阶段来说,按照时间顺序可以将mapreduce分为以下几个阶段：</p>
<ul>
<li>map</li>
<li>partition</li>
<li>combiner</li>
<li>shuffle</li>
<li>reduce</li>
</ul>
<p>mapreduce的运行大致的流程图如下:<br><img src="/images/hadoop/mapreduce.png" alt=""></p>
<p>1 数据输入端<br><br>    每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M）为一个分片,当然我们也可以设置块的大小。map输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为100M，由io.sort.mb属性控制）,当该缓冲区快要溢出时（默认为缓冲区大小的80%，由io.sort.spill.percent属性控制）,会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件</p>
<p>2 分区阶段<br><br>    在写入磁盘之前,线程首先根据reduce任务的数目将数据划分为相同数目的分区,也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据。其实分区就是对数据进行hash的过程。然后对每个分区中的数据进行排序,如果此时设置了combia，将排序后的结果进行combia操作，这样做的目的是让尽可能少的数据写入到磁盘。</p>
<p>3 合并阶段<br><br>    当map任务输出最后一个记录时，可能会有很多的溢写文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和combia操作，目的有两个：1.尽量减少每次写入磁盘的数据量；2.尽量减少下一复制阶段网络传输的文件数量。最后合并成了一个已分区且已排序的文件。为了减少网络传输的文件数量，这里可以将数据压缩，只要将mapred.compress.map.out设置为true就可以了</p>
<p>4 shuffle阶段<br></p>
<pre><code>shuffle阶段主要分为：拷贝,合并两个阶段。
拷贝阶段：去那台机器拷贝那些数据?因此在map阶段结束以后的数据主要是进行相应分区的数据,
这些数据的根据分区进行相应hash取模运行,计算出每个分区的数据应该拷贝给那个reduces上进行汇总计算。
合并阶段:也就是merge阶段,这个阶段有3种形式：
                1) 内存中合并 
                2）内存文件合并写入到磁盘 
                3) 将磁盘中文件合并&lt;br&gt;
当内存缓存池中的数据量达到一定的阈值以后,与map端类似,这里的缓冲区大小要比map端的更为灵活,
它基于JVM的heap size设置,当内存中的大小达到上限后进行相应的溢写,将数据写入到文件磁盘中。
</code></pre><p>Shuffle中的流程图如下：<br>    <img src="/images/hadoop/shuffle.jpg" alt=""></p>
<p>5 reduce阶段<br><br>    结合上面的图可以看到,在reduce的输入可以看到有一个or,也就是说这个reduce的数据输入来源可以死磁盘和内存,数据进行过shuffler过程拷贝过来进行合并,如果拷贝和合并后的数据文件大小都没有超过内存缓冲池的阈值就不会出现溢写磁盘的动作,同样,在内存中,拷贝过来的文件有必要进行合并的动作嘛？从这个图中看应该是没有进行合并的动作,当然现在还是在学习中,没有具体的研究~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/23/hadoop-study002/" itemprop="url">
                  Hadoop学习笔记(二) Hadoop的搭建和WordCount程序的开发
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-23T10:53:55+08:00" content="2016-04-23">
              2016-04-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">hadoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/23/hadoop-study002/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/23/hadoop-study002/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hadoop单节点的搭建"><a href="#hadoop单节点的搭建" class="headerlink" title="hadoop单节点的搭建"></a>hadoop单节点的搭建</h2><h3 id="下载hadoop："><a href="#下载hadoop：" class="headerlink" title="下载hadoop："></a>下载hadoop：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget http://apache.fayea.com/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz</div><div class="line">tar -zxvf hadoop-2.7.1.tar.gz</div></pre></td></tr></table></figure>
<h3 id="解压配置免密码登陆"><a href="#解压配置免密码登陆" class="headerlink" title="解压配置免密码登陆:"></a>解压配置免密码登陆:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">//生成秘钥</div><div class="line">ssh-keygen -t rsa</div><div class="line"></div><div class="line">//一直回车,在当前目录中会出现2个文件,一个是公钥,一个是私钥</div><div class="line">id_rsa:			私钥</div><div class="line">id_rsa.pub:		公钥</div><div class="line"></div><div class="line">//创建认证文件</div><div class="line">cat id_rsa.pub &gt;&gt; authorized_keys</div></pre></td></tr></table></figure>
<h3 id="配置hosts"><a href="#配置hosts" class="headerlink" title="配置hosts"></a>配置hosts</h3><p>主要是修改其中的hostname和对应的ip,修改如下:</p>
<center><br>    <img src="/images/hadoop/hosts.jpg" alt=""><br></center>

<h3 id="配置相应的配置文件"><a href="#配置相应的配置文件" class="headerlink" title="配置相应的配置文件"></a>配置相应的配置文件</h3><p>进入到hadoop的解压目录中,其中etc/hadoop目录是配置目录,进入到目录中可以看到:</p>
<center><br>    <img src="/images/hadoop/conf-list.jpg" alt=""><br></center>

<p>其中主要配置的是环境变量(hadoop-env.sh)和core-site.xml,hdfs-site.xml,mapred.site.xml以及yarn-site.xml则五个文件,相关的配置如下:</p>
<p>hadoop-env.sh:</p>
<center><br>    <img src="/images/hadoop/hadoop-env.jpg" alt=""><br></center>

<p>core-site.xml:</p>
<center><br>    <img src="/images/hadoop/core-site.jpg" alt=""><br></center>

<p>hdfs-site.xml:</p>
<center><br>    <img src="/images/hadoop/hdfs-site.jpg" alt=""><br></center>

<p>mapred-site.xml:</p>
<center><br>    <img src="/images/hadoop/mapred-site.jpg" alt=""><br></center>

<p>yarn-site.sh:</p>
<center><br>    <img src="/images/hadoop/yarn-site.jpg" alt=""><br></center>

<h3 id="运行hadoop"><a href="#运行hadoop" class="headerlink" title="运行hadoop"></a>运行hadoop</h3><p>配置好以后,需要先格式化一下,格式的命令如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop namenode -format</div></pre></td></tr></table></figure>
<p>格式化NameNode的动作,主要做了一下一下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">* 如果dfs.namenode.name.dir对应的文件夹目录不存在,则创建一个目录,并初始化fsimage,和edits并且写入一些初始值,这个动作在文件系统中一样,格式化主要是清空重置。</div><div class="line">* 如果对应的目录地址的数据已经存在了,则删除相应的,目录下的文件,在重新建立</div></pre></td></tr></table></figure>
<p>格式完hadoop以后就可以启动hadoop了,启动的命令如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /usr/loca/share/hadoop2.7.1</div><div class="line"></div><div class="line">./sbin/start-all.sh</div></pre></td></tr></table></figure>
<p>启动过后的进程数据如下：</p>
<center><br>    <img src="/images/hadoop/jps.jpg" alt=""><br></center>

<center><br>    <img src="/images/hadoop/cluster-view.jpg" alt=""><br></center>



<h2 id="hadoop的wordcount的程序的开发"><a href="#hadoop的wordcount的程序的开发" class="headerlink" title="hadoop的wordcount的程序的开发"></a>hadoop的wordcount的程序的开发</h2><p>代码结构如下:</p>
<center><br>    <img src="/images/hadoop/code-tree.jpg" alt="流程图"><br></center>


<p>自定义mapper类代码如下:</p>
<center><br>    <img src="/images/hadoop/mapper.jpg" alt="流程图"><br></center>


<p>自定义reducer类代码如下:</p>
<center><br>    <img src="/images/hadoop/reduce.jpg" alt="流程图"><br></center>

<p>wordcount程序的数据源：</p>
<center><br>    <img src="/images/hadoop/wordcount-data.jpg" alt=""><br></center>

<p>wordcount程序的执行过程：</p>
<center><br>    <img src="/images/hadoop/wordcount.jpg" alt=""><br></center>

<p>wordcount程序的结果展示:</p>
<center><br>    <img src="/images/hadoop/wordcount-result.jpg" alt=""><br></center>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/20/Hadoop-study001/" itemprop="url">
                  Hadoop学习笔记(一) 基础知识介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-20T10:53:55+08:00" content="2016-04-20">
              2016-04-20
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">hadoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/20/Hadoop-study001/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/20/Hadoop-study001/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>1 HDFS介绍</p>
<pre><code>Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data.
翻译过来就是:一种提供高吞吐量访问应用数据的分布式文件系统

hdfs是hadoop的基础,主要用于数据的存储,在数据存储的基础上,MapReduce程序等就可以在相应的分布式节点上进行数据的处理,HDFS简单点就是文件系统,比如操作系统中的文件系统等,文件系统对外提供文件的各种操作,比如读写,创建,删除等等操作,同时文件系统中还有文件的属主等等权限问题,这些在HDFS中都有。

HDFS运行机制
* 一个名字节点和多个数据节点
* 数据复制(副本，冗余机制)
* 故障检测
    -- 数据节点
        心跳包(检验是否宕机)
        块报告(安全模式下检测)
        数据完整性检测(校验和比较)
    -- 名字节点
        日志文件，镜像文件
* 空间回收机制

HDFS优点
* 高容错性
    数据自动保存多个副本
    副本丢失自动回复
* 适合批处理
    移动计算而非数据
    数据位置暴露给计算框架
* 适合大数据处理
    GB,TB,PB级别数据
* 可以构建在廉价的机器上

HDFS缺点
* 低延迟数据访问
    比如毫秒级别,毫秒级相应的数据是不适合在hadoop中存储的
    低延迟和高吞吐率
* 小文件存取
    占用NameNode大量内存
    寻道时间超过读取时间
* 并发写入，文件随机修改
    一个文件只能有一个写者
    仅支持append

HDFS数据存储单元(block)
* 文件被切分成固定大小的数据块
    默认数据块大小为64M,可配置
    若文件大小不到64M，可单独存成一个block
* 一个文件存储方式
    按照大小切分成若干个block,存储在不同的节点上
    默认情况下每个block都有上个副本集，这个需要配置
    配置副本集的方式: 设置dfs.replication值
* Block大小和副本数通过client端上传文件的时候进行设置,文件上成功以后副本数可以改变,BlockSize不可变
</code></pre><p>2 NameNode</p>
<pre><code>NameNode管理文件系统的命名空间。它维护着文件系统树及整棵树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件和编辑日志文件。NameNode也记录着每个文件中各个块所在的数据节点信息，但它并不永久保存块的位置信息，因为这些信息在系统启动时由数据节点重建

- NameNode 主要功能:接收客户端的读写请求服务
- NameNode 保存metadata信息
    * 文件owership和permissions(文件权限,所有者等)
    * 文件包含哪些块
    * Block保存在哪个DataNode(由DataNode启动时上报)
- NameNode的metadata信息,在启动后会加载到内存中 
    * metadata存储到磁盘文件名为：fsimage
    * block的位置信息不会保存到fsimage
    * edits记录对metadata的操作信息

NameNode的处理流程:
-启动集群,初始化读取fsimage,将相应的文件的元数据和相应的权限数据以及分布节点数据加载到内存中
-接收到客户端的操作读写请求,将接收到的客户端请求到写入到edits中,同时这个时候文件目录数据都已经在内存中改过了,同时定期会将操作流水也就是edits合并到fsimage中,如果这个合并的动作也需要让NameNode来进行操作,对NameNode的压力比较大,因此SecondaryNameNode来进行这个操作,同时可以进行数据的备份。
</code></pre><p>3 SecondaryNameNode(SNN)</p>
<pre><code>- 它不是nn的备份(但是可以做备份),它主要的工作是帮助nn合并edits-log,减少nn启动的时间
- SNN执行合并时机
    根据配置文件设置的时间间隔fs.checkpoint.period 默认3600秒
    根据配置文件设置edits.log大小 fs.checkpoint.size规定edits文件的最大默认是64M
 主要是因为namenode节点的主要任务是用于接收客户端的读写请求服务，因此如果namenode的主要任务用于去进行对edits进行合并，也就是客户端读写请求的日志文件的合并操作,会浪费cpu资源，因此独立出来一个SecondaryNameNode进行操作日志的合并动作

SNN合并流程
* 在namenode上的edits文件和fsimage进行合并：
* secondNamenode 从namenode中拷贝edits和fsimage拷贝过来,由于在secondnamenode
* 进行拷贝合并的时候,namenode还需要继续进行接收客户端发过来的请求,因此会生产性的edits用户操作日志文件
* 用于写行接收到的。
* 拷贝过来以后新生成的edits文件用于接收用户操作日志,那么secondnamenode相当于针对namenode的冷备份
* secondnamenode拷贝edits和fsimage文件后,针对这两个文件进行合并成行的fsimage,并将这个文件拷贝到namenode上
* 用户更新fsimage
</code></pre><p>SecondaryNameNode的合并流程图：<br>流程图如下:<br><img src="/images/hadoop/namenode.jpg" alt="流程图"></p>
<p>3 DataNode</p>
<pre><code>主要作用:
* 存储数据(Block)
* 启动DataName线程的时候会向NameNode汇报Block信息
* 通过向Namenode发送心跳包保持与其的联系(3秒一次),如果十分钟没有收到DataName的心跳
* 认为其已经lost,也就是挂了,NameNode往DataNode发送消息，告诉它们哪些文件副本数不够,DataNode上根据剩余的副本数其中的一个拷贝一份
* 发送到其他的机器上,也就是挂了一台DataNode,将副本数不够的block拷贝补足副本数

Block的放置策略
* 第一个副本：放置在上传文件的DataNode
    如果是集群外提交的,则随机挑选一台，磁盘不太满，cpu不太忙的节点
* 第二个副本:放置在于第一个副本不同的机架节点上

* 第三个副本：与第二个副本相同的机架的节点上
* 更多副本：随机节点
</code></pre><p>4 HDFS读流程</p>
<pre><code>HDFSClient --&gt;  代用Distributed FileSystem接口打开文件 
           --&gt;    Distributed FileSystem接口从namenode获取block的地址信息
           --&gt;    拿到相应的block位置信息后,找到副本所在的位置信息,找一台比较空闲的机器进行读取信息,至于那些机器是空闲的,DataNode会向NameNode发送消息,上报自己的情况,也就是NameNode知道DataNode的状态
           --&gt;    同时可以并行的读取其他block的数据,最后在client进行合并文件
           --&gt;  合并好了后关闭这个文件流
</code></pre><p>读取流程图如下:<br><img src="/images/hadoop/hdfs-reader.png" alt="读取流程图"></p>
<p>5 HDFS写流程</p>
<pre><code>HDFSClient --&gt;  代用Distributed FileSystem接口创建接口,传递文件名,文件大小,文件所有者等信息给NameNode
           --&gt;  NameNode向client返回时需要切分的block数据块以及对应的每个block存储的位置
           --&gt;    client将block根据NameNode发送过来的DataNode地址写入到相应的节点上
           --&gt;    由于每个block有副本集,因此,在写入这个数据的同时,写入的block那个节点会进行数据的复制到其他DataNode
           --&gt;    DataNode复制完数据以后会返回数据给client
           --&gt;    client会上报处理完成给NameNode
</code></pre><p>读取流程图如下:<br><img src="/images/hadoop/hdfs-writer.jpg" alt="写取流程图"></p>
<p>6 安全模式</p>
<pre><code>- namenode启动时候,首先将映像问价(fsimage)载入到内存,并执行编辑日志中的各项操作
- 一旦在内存中成功建立文件系统元数据的映像,则创建一个新的fsimage文件(这个操作不需要SecondaryNameNode)和一个空的编辑日志文件
- 此刻namenode运行在安全模式,即namenode的文件系统对于客户端来说是只读的(显示目录，显示文件的内容等，写删除重命名等操作会失败)
- 在此阶段NameNode收集各个DataNode的报告，当数据库达到了直销副本数以上时,会被认为是安全的,在一定的数据库被劝人为安全后,在过若干时间，安全模式结束
- 当检测到副本数不足的数据块时,该数据块会被复制直到达到最小副本数,系统中数据块的位置并不是由NameNode维护的,而是以块列的形式存储在DataNode中
</code></pre><p>7 Hadoop核心组件–MR(MapReduce)</p>
<pre><code>- Hadoop 分布式计算框架(MapReduce)
- MapReduce设计理念
    * 何为分布式计算
    * 移动计算而不是移动数据
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/18/mysql-study003/" itemprop="url">
                  mysql学习笔记(三)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-18T10:02:05+08:00" content="2016-04-18">
              2016-04-18
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/mysql/" itemprop="url" rel="index">
                    <span itemprop="name">mysql</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/18/mysql-study003/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/18/mysql-study003/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="mysql学习中的存储过程和定时任务"><a href="#mysql学习中的存储过程和定时任务" class="headerlink" title="mysql学习中的存储过程和定时任务"></a>mysql学习中的存储过程和定时任务</h1><p>mysql中定时任务的创建：<br></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">DELIMITER ;;</div><div class="line">CREATE PROCEDURE `proc_move_data`(in vday varchar(50))</div><div class="line">begin</div><div class="line">	declare v_sql varchar(3000); </div><div class="line">	/**</div><div class="line">		设定数据库表名</div><div class="line">	**/</div><div class="line">	declare tableName varchar(100); </div><div class="line">		</div><div class="line">	set tableName = concat(&apos;good_db_&apos;,replace(vday,&apos;-&apos;,&apos;&apos;));</div><div class="line"></div><div class="line">	/**</div><div class="line">		判断数据库表是否已经存在</div><div class="line">		1 如果存在，则清空数据</div><div class="line">		2 如果不存在，则创建表</div><div class="line">	**/</div><div class="line">	set v_sql = concat(&apos;create table &apos;,tableName,&apos;(</div><div class="line">	id varchar(32),dt datetime,site_id int,school_id bigint,category_id bigint,category_name varchar(100),</div><div class="line">	good_id bigint,good_name varchar(100),price decimal(18,2),sale_num int,primary key(id)</div><div class="line">	)&apos;);	</div><div class="line">		</div><div class="line">	/**</div><div class="line">		###注意很重要，将连成成的字符串赋值给一个变量（可以之前没有定义，但要以@开头）</div><div class="line">	**/				</div><div class="line">	set @v_sql=v_sql;   </div><div class="line">    prepare stmt from @v_sql;  </div><div class="line">    EXECUTE stmt;      </div><div class="line">end;;</div><div class="line">DELIMITER ;</div></pre></td></tr></table></figure>
<p>这个地方就是进行存储过程的创建，同样的在存储过程中可以直接使用sql语句进行操作。</p>
<p>定时任务的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">create event event_task </div><div class="line">on schedule every 30 second </div><div class="line">on completion preserve </div><div class="line">do call proc_move_data();</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
           <!-- 多说热评文章 start -->
	<div class="ds-top-threads" data-range="daily" data-num-items="5"></div>
<!-- 多说热评文章 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"sunxing"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->


           <section id="comments">
   <!-- 多说评论框 start -->
<div id="ds-thread" class="ds-thread" data-thread-key="<%= post.path %>" data-title="<%= post.title %>" data-url="<%= post.permalink %>"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"sunxing"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->
  </section>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/default_avatar.jpg"
               alt="孙星" />
          <p class="site-author-name" itemprop="name">孙星</p>
          <p class="site-description motion-element" itemprop="description">sunxing's blog | mysql | java | scala | hadoop | spark | kafka | flume</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">78</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">孙星</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"sunxing"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  

  

  
<script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js">
</script>


</body>
</html>
